{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "## Predict Product Prices\n",
        "An introduction to LoRA and QLoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Hyperparameters for QLoRA Fine-Tuning\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Log in to HuggingFace\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Trying out different Quantization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Base Model without quantization\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "ElUPTnB28iNK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2VcoS_DY9YSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "j6gJWw3r86KQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09566ff-c45d-4ea8-c150-e1562eac07f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 4.9 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "sVf8hf6S88C9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad8396b-883f-42a5-fc6c-d238c2be41dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (installs and imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ],
      "metadata": {
        "id": "Gv-00uzNFPOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Base Model using 8 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "7ycR0B4CzUSJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CVErveOYFOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "8FDztdnv0RCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f051c1ea-7dba-41a7-fc1b-b43f0ed9e6e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 1.5 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "IvqOxYfk0RnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5a40e1-06c2-4c84-d4a8-e13f24b77be3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ],
      "metadata": {
        "id": "Pb7-0OfSEycl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Base Model using 4 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "1FfMJ2JbzEr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499017ea-b5de-489e-d947-6b2c11fea9b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 1.01 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "mjp1EHH10WTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c47eade-f17c-43d3-998c-8cecc0ee88a8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "# Let's count the number of weights using their dimensions:\n",
        "\n",
        "# See the matrix dimensions above\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# Each layer comes to\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# There are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "# So the total size in MB is\n",
        "size = (params * 4) / 1_000_000\n",
        "\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ],
      "metadata": {
        "id": "IIaqo-gyBQRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e35fb4-f7b8-49fa-924a-2ba302e6bc41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of params: 27,262,976 and size 109.1MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKnCsfUQBG-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}